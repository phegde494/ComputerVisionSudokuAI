{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c53d564",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import cv2\n",
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df27a1b",
   "metadata": {},
   "source": [
    "After importing the required libraries, we next load the MNIST data set to train our digit recognition model\n",
    "This dataset has thousands of 28x28 black and white digit images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "094785a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize pixel vals to doubles in range [0, 1]\n",
    "\n",
    "x_train = x_train.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "x_test = x_test.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "num_classes = 10\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1264d15",
   "metadata": {},
   "source": [
    "Next, we define our digit recognition model to take in 28x28 pixel images\n",
    "The ReLU function optimizes the hidden layer while Softmax optimizes the output layer (good for multi-class classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f188cd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "digit_recognition_model = keras.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.Dropout(0.25),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(num_classes, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13296252",
   "metadata": {},
   "source": [
    "Finally, we can train the model and save the final product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be05aae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "469/469 [==============================] - 42s 88ms/step - loss: 0.1582 - accuracy: 0.9535 - val_loss: 0.4076 - val_accuracy: 0.8851 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "469/469 [==============================] - 43s 91ms/step - loss: 0.0616 - accuracy: 0.9817 - val_loss: 0.0561 - val_accuracy: 0.9819 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "469/469 [==============================] - 43s 93ms/step - loss: 0.0476 - accuracy: 0.9854 - val_loss: 0.0370 - val_accuracy: 0.9878 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "469/469 [==============================] - 43s 91ms/step - loss: 0.0385 - accuracy: 0.9884 - val_loss: 0.0327 - val_accuracy: 0.9890 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "469/469 [==============================] - 44s 94ms/step - loss: 0.0311 - accuracy: 0.9902 - val_loss: 0.0424 - val_accuracy: 0.9864 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "469/469 [==============================] - 44s 93ms/step - loss: 0.0315 - accuracy: 0.9900 - val_loss: 0.0303 - val_accuracy: 0.9891 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "469/469 [==============================] - 40s 86ms/step - loss: 0.0250 - accuracy: 0.9920 - val_loss: 0.0352 - val_accuracy: 0.9896 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "469/469 [==============================] - 41s 88ms/step - loss: 0.0213 - accuracy: 0.9930 - val_loss: 0.0328 - val_accuracy: 0.9892 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "469/469 [==============================] - ETA: 0s - loss: 0.0190 - accuracy: 0.9940\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "469/469 [==============================] - 43s 91ms/step - loss: 0.0190 - accuracy: 0.9940 - val_loss: 0.0362 - val_accuracy: 0.9889 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "469/469 [==============================] - 43s 92ms/step - loss: 0.0139 - accuracy: 0.9955 - val_loss: 0.0244 - val_accuracy: 0.9924 - lr: 1.0000e-04\n",
      "Epoch 11/50\n",
      "469/469 [==============================] - 43s 91ms/step - loss: 0.0104 - accuracy: 0.9967 - val_loss: 0.0247 - val_accuracy: 0.9926 - lr: 1.0000e-04\n",
      "Epoch 12/50\n",
      "469/469 [==============================] - 45s 96ms/step - loss: 0.0086 - accuracy: 0.9974 - val_loss: 0.0246 - val_accuracy: 0.9924 - lr: 1.0000e-04\n",
      "Epoch 13/50\n",
      "469/469 [==============================] - ETA: 0s - loss: 0.0072 - accuracy: 0.9979\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "469/469 [==============================] - 43s 91ms/step - loss: 0.0072 - accuracy: 0.9979 - val_loss: 0.0248 - val_accuracy: 0.9922 - lr: 1.0000e-04\n",
      "Epoch 14/50\n",
      "469/469 [==============================] - 42s 89ms/step - loss: 0.0073 - accuracy: 0.9976 - val_loss: 0.0247 - val_accuracy: 0.9921 - lr: 1.0000e-05\n",
      "Epoch 15/50\n",
      "469/469 [==============================] - 42s 90ms/step - loss: 0.0073 - accuracy: 0.9978 - val_loss: 0.0245 - val_accuracy: 0.9920 - lr: 1.0000e-05\n",
      "Epoch 16/50\n",
      "469/469 [==============================] - ETA: 0s - loss: 0.0068 - accuracy: 0.9982\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "469/469 [==============================] - 41s 88ms/step - loss: 0.0068 - accuracy: 0.9982 - val_loss: 0.0244 - val_accuracy: 0.9921 - lr: 1.0000e-05\n",
      "Epoch 17/50\n",
      "469/469 [==============================] - 41s 88ms/step - loss: 0.0065 - accuracy: 0.9979 - val_loss: 0.0244 - val_accuracy: 0.9921 - lr: 1.0000e-06\n",
      "Epoch 18/50\n",
      "469/469 [==============================] - 41s 87ms/step - loss: 0.0064 - accuracy: 0.9980 - val_loss: 0.0245 - val_accuracy: 0.9920 - lr: 1.0000e-06\n",
      "Epoch 19/50\n",
      "469/469 [==============================] - ETA: 0s - loss: 0.0065 - accuracy: 0.9983\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "469/469 [==============================] - 42s 89ms/step - loss: 0.0065 - accuracy: 0.9983 - val_loss: 0.0244 - val_accuracy: 0.9920 - lr: 1.0000e-06\n",
      "Epoch 20/50\n",
      "469/469 [==============================] - 41s 87ms/step - loss: 0.0067 - accuracy: 0.9981 - val_loss: 0.0244 - val_accuracy: 0.9921 - lr: 1.0000e-07\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 0.0244 - accuracy: 0.9924\n",
      "Test Loss: 0.02436276711523533\n",
      "Test Accuracy: 0.9923999905586243\n",
      "(None, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "# Train model \n",
    "digit_recognition_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1)\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 20\n",
    "digit_recognition_model.fit(\n",
    "    x_train, y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=(x_test, y_test),\n",
    "    callbacks=[early_stopping, reduce_lr]\n",
    ")\n",
    "test_loss, test_accuracy = digit_recognition_model.evaluate(x_test, y_test)\n",
    "\n",
    "digit_recognition_model.save(\"trained_digit_model.h5\")\n",
    "\n",
    "print('Test Loss:', test_loss)\n",
    "print('Test Accuracy:', test_accuracy)\n",
    "print(digit_recognition_model.input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ea27a0",
   "metadata": {},
   "source": [
    "Now, let's try running our model on the following example digit image:"
   ]
  },
  {
   "attachments": {
    "digit2.jpg": {
     "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+vTPDHwP8TeJ9DtdXiuLCzt7kbo0uWcOU7NgKRgjkc81i+O/hvrPgW8xco1zp7ELHfIm1HYqCRjJIPUc9cHFcbSgEnABJ9BXaafH8Rrrw3NpdjBrkmjohLQLE/l7c5OOPUHgV6Fcw3um/sxXNt4hZo7qW5X7FDdLtlRfOU7QG5zgSH/dPpXhFel/Bzxj4a8H6vfzeILZy86ILe6WLzPI27i3HUZ+XkA9PQ16Pc/Hfw7pM91LaXusa20wDRxSQRQww9eAdob35DfWuNg+Ny67Dfab430SDUNLuQxjW2UK8BwcAZPPOPmyCOvPSvH6KKKK//9k="
    }
   },
   "cell_type": "markdown",
   "id": "ea38847f",
   "metadata": {},
   "source": [
    "![digit2.jpg](attachment:digit2.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71c8cfab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "ex_img = cv2.imread('images/digit2.jpg')\n",
    "ex_img = cv2.cvtColor(ex_img, cv2.COLOR_BGR2GRAY)\n",
    "ex_img = cv2.resize(ex_img, (28, 28))\n",
    "ex_img = ex_img.astype('float32') / 255.0\n",
    "ex_img = np.reshape(ex_img, (1, 28, 28, 1))\n",
    "print (ex_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d6686f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 110ms/step\n",
      "[[2.2001387e-08 9.4310391e-09 9.9999988e-01 3.6452267e-08 2.0942670e-09\n",
      "  3.5546115e-11 7.8091346e-11 4.2853888e-08 4.8846484e-08 1.2023473e-09]]\n",
      "Classified digit is: 2\n"
     ]
    }
   ],
   "source": [
    "digit_probabilities = digit_recognition_model.predict(ex_img)\n",
    "\n",
    "print(digit_probabilities)\n",
    "\n",
    "digit = np.argmax(digit_probabilities)\n",
    "\n",
    "print(\"Classified digit is: \" + str(digit))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0488f0",
   "metadata": {},
   "source": [
    "Next, let's try running it on another example digit image. This time, the image has been extracted from the board after our transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af6606ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 28, 28, 1)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/P9b71AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAaiElEQVR4nO3df2xV9f3H8del0Atie1mp7e0dpRYUMKJsY9I1KupogG4xICTDH3/AQjBgMULnNCwqMpd0Y4kzmg7/cXQmgs5EIJKMBIqU4AqOKiHE2VDWSU1/ICy9txS4dPTz/YN4v15pxXO5t+/ey/ORnMTeez69b483fXra01Ofc84JAIAhNsJ6AADA9YkAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEyOtB/im/v5+tbe3KycnRz6fz3ocAIBHzjn19PQoFAppxIjBz3OGXYDa29tVXFxsPQYA4Bq1tbVpwoQJgz4/7L4Fl5OTYz0CACAJrvb1PGUBqq2t1c0336zRo0errKxMH3300Xdax7fdACAzXO3reUoC9M4776i6ulrr16/Xxx9/rBkzZmjevHk6depUKl4OAJCOXArMmjXLVVVVxT6+dOmSC4VCrqam5qprw+Gwk8TGxsbGluZbOBz+1q/3ST8DunjxopqamlRRURF7bMSIEaqoqFBjY+MV+0ejUUUikbgNAJD5kh6g06dP69KlSyosLIx7vLCwUJ2dnVfsX1NTo0AgENu4Ag4Arg/mV8GtW7dO4XA4trW1tVmPBAAYAkn/PaD8/HxlZWWpq6sr7vGuri4Fg8Er9vf7/fL7/ckeAwAwzCX9DCg7O1szZ85UfX197LH+/n7V19ervLw82S8HAEhTKbkTQnV1tZYuXaof//jHmjVrll555RX19vbql7/8ZSpeDgCQhlISoCVLlujLL7/UCy+8oM7OTv3gBz/Qrl27rrgwAQBw/fI555z1EF8XiUQUCASsxwAAXKNwOKzc3NxBnze/Cg4AcH0iQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJkZaDwB7I0fyNoCN/v7+IVmD4YkzIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABHehzDA+n8/zmqeeeioFkwBX19ra6nnN9u3bPa/hBqbDE2dAAAATBAgAYCLpAXrxxRfl8/nitmnTpiX7ZQAAaS4lPwO6/fbbtWfPnv9/Ef7gGQDgG1JShpEjRyoYDKbiUwMAMkRKfgZ0/PhxhUIhTZo0SY899phOnjw56L7RaFSRSCRuAwBkvqQHqKysTHV1ddq1a5c2bdqk1tZW3Xvvverp6Rlw/5qaGgUCgdhWXFyc7JEAAMOQzznnUvkC3d3dKikp0csvv6zly5df8Xw0GlU0Go19HIlEiNA1SOT3gKqrq1MwCXB1/B5QZguHw8rNzR30+ZRfHTBu3DhNmTJFLS0tAz7v9/vl9/tTPQYAYJhJ+e8BnT17VidOnFBRUVGqXwoAkEaSHqCnn35aDQ0N+s9//qN//OMfeuihh5SVlaVHHnkk2S8FAEhjSf8W3BdffKFHHnlEZ86c0U033aR77rlHBw8e1E033ZTslwIApLGUX4TgVSQSUSAQsB4jbWVlZXle09fXl4JJMBwkclHKUDpw4IDnNXPnzvW85vz5857X4Npd7SIE7gUHADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJhI+R+kw9BK5N6yifxVSqSH0tLShNYN1U1Mh9m9kDHEOAMCAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACe6GnWH6+/s9r5k8eXIKJsFw0NfXl9C6kSO9f2lI5M7Wr732muc10WjU8xoMT5wBAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmuBkpkCZycnKG7LUSuYnplClTPK/5/PPPPa9J5KanGJ44AwIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATHAzUsBASUmJ5zUNDQ2e12RlZXleI0l/+ctfPK9pb2/3vIYbi17fOAMCAJggQAAAE54DtH//fj344IMKhULy+Xzavn173PPOOb3wwgsqKirSmDFjVFFRoePHjydrXgBAhvAcoN7eXs2YMUO1tbUDPr9x40a9+uqrev3113Xo0CGNHTtW8+bN04ULF655WABA5vB8EUJlZaUqKysHfM45p1deeUXPPfecFixYIEl68803VVhYqO3bt+vhhx++tmkBABkjqT8Dam1tVWdnpyoqKmKPBQIBlZWVqbGxccA10WhUkUgkbgMAZL6kBqizs1OSVFhYGPd4YWFh7LlvqqmpUSAQiG3FxcXJHAkAMEyZXwW3bt06hcPh2NbW1mY9EgBgCCQ1QMFgUJLU1dUV93hXV1fsuW/y+/3Kzc2N2wAAmS+pASotLVUwGFR9fX3ssUgkokOHDqm8vDyZLwUASHOer4I7e/asWlpaYh+3trbqyJEjysvL08SJE7VmzRr97ne/06233qrS0lI9//zzCoVCWrhwYTLnBgCkOc8BOnz4sB544IHYx9XV1ZKkpUuXqq6uTs8884x6e3v1+OOPq7u7W/fcc4927dql0aNHJ29qAEDa87lhdjfASCSiQCBgPQaQUj09PZ7XjB071vOaaDTqeY2U2M1ST506ldBrIXOFw+Fv/bm++VVwAIDrEwECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEx4/nMMAOL94he/8Lzmhhtu8LzG5/N5XlNUVOR5jSR1d3cntA7wgjMgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAENyMFrtFLL73keU0iNxZ94403PK85e/as5zXAUOEMCABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwwc1Iga9ZuXKl5zWTJ0/2vKavr8/zmg8//NDzmv7+fs9rgKHCGRAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYMLnnHPWQ3xdJBJRIBCwHgPXqU8//dTzmmnTpnle09bW5nnNbbfd5nnNuXPnPK8BkiUcDis3N3fQ5zkDAgCYIEAAABOeA7R//349+OCDCoVC8vl82r59e9zzy5Ytk8/ni9vmz5+frHkBABnCc4B6e3s1Y8YM1dbWDrrP/Pnz1dHREdu2bt16TUMCADKP57+IWllZqcrKym/dx+/3KxgMJjwUACDzpeRnQPv27VNBQYGmTp2qVatW6cyZM4PuG41GFYlE4jYAQOZLeoDmz5+vN998U/X19frDH/6ghoYGVVZW6tKlSwPuX1NTo0AgENuKi4uTPRIAYBi6pt8D8vl82rZtmxYuXDjoPv/+9781efJk7dmzR3PmzLni+Wg0qmg0Gvs4EokQIZjh94CA5DH/PaBJkyYpPz9fLS0tAz7v9/uVm5sbtwEAMl/KA/TFF1/ozJkzKioqSvVLAQDSiOer4M6ePRt3NtPa2qojR44oLy9PeXl52rBhgxYvXqxgMKgTJ07omWee0S233KJ58+YldXAAQHrzHKDDhw/rgQceiH1cXV0tSVq6dKk2bdqko0eP6q9//au6u7sVCoU0d+5cvfTSS/L7/cmbGgCQ9rgZKfA1fX19nteMHOn5/+P02WefeV5zxx13eF7zv//9z/MaIFnML0IAAGAgBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMOH9Nr5AGtiwYUNC67KyspI8ycDuu+8+z2u4szUyDWdAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJbkaKjLRmzZqE1vl8Ps9rnnjiCc9rTp8+7XkNkGk4AwIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATHAzUgx7P/zhDz2v8fv9KZhkYCdPnvS8xjmXgkmA9MIZEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABggpuRYth79NFHPa/Jzs5O6LWam5s9r/nnP//peQ03IwU4AwIAGCFAAAATngJUU1Oju+66Szk5OSooKNDChQuv+JbFhQsXVFVVpfHjx+vGG2/U4sWL1dXVldShAQDpz1OAGhoaVFVVpYMHD2r37t3q6+vT3Llz1dvbG9tn7dq1ev/99/Xuu++qoaFB7e3tWrRoUdIHBwCkN08XIezatSvu47q6OhUUFKipqUmzZ89WOBzWG2+8oS1btuinP/2pJGnz5s267bbbdPDgQf3kJz9J3uQAgLR2TT8DCofDkqS8vDxJUlNTk/r6+lRRURHbZ9q0aZo4caIaGxsH/BzRaFSRSCRuAwBkvoQD1N/frzVr1ujuu+/W9OnTJUmdnZ3Kzs7WuHHj4vYtLCxUZ2fngJ+npqZGgUAgthUXFyc6EgAgjSQcoKqqKh07dkxvv/32NQ2wbt06hcPh2NbW1nZNnw8AkB4S+kXU1atXa+fOndq/f78mTJgQezwYDOrixYvq7u6OOwvq6upSMBgc8HP5/X75/f5ExgAApDFPZ0DOOa1evVrbtm3T3r17VVpaGvf8zJkzNWrUKNXX18cea25u1smTJ1VeXp6ciQEAGcHTGVBVVZW2bNmiHTt2KCcnJ/ZznUAgoDFjxigQCGj58uWqrq5WXl6ecnNz9eSTT6q8vJwr4AAAcTwFaNOmTZKk+++/P+7xzZs3a9myZZKkP/3pTxoxYoQWL16saDSqefPm6c9//nNShgUAZA5PAfouN1AcPXq0amtrVVtbm/BQwNeNGTNmyF7ryy+/9Lzmv//9bwomATIf94IDAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACAiYT+IiqQqfr7+4dkDQDOgAAARggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAE9yMFEMqLy/P85rly5d7XuPz+TyvATC0OAMCAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAExwM1IMqYsXL3pe097e7nnNpEmTPK+RpB07dnhe45xL6LWA6x1nQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACW5GiiHV29vrec3Pf/5zz2uCwaDnNZLU1NTkeQ03IwUSwxkQAMAEAQIAmPAUoJqaGt11113KyclRQUGBFi5cqObm5rh97r//fvl8vrht5cqVSR0aAJD+PAWooaFBVVVVOnjwoHbv3q2+vj7NnTv3iu/rr1ixQh0dHbFt48aNSR0aAJD+PF2EsGvXrriP6+rqVFBQoKamJs2ePTv2+A033JDwD4EBANeHa/oZUDgcliTl5eXFPf7WW28pPz9f06dP17p163Tu3LlBP0c0GlUkEonbAACZL+HLsPv7+7VmzRrdfffdmj59euzxRx99VCUlJQqFQjp69KieffZZNTc367333hvw89TU1GjDhg2JjgEASFM+l+AvMaxatUp///vfdeDAAU2YMGHQ/fbu3as5c+aopaVFkydPvuL5aDSqaDQa+zgSiai4uDiRkZAGfD6f5zVTp071vGYofw+op6cnodcCMl04HFZubu6gzyd0BrR69Wrt3LlT+/fv/9b4SFJZWZkkDRogv98vv9+fyBgAgDTmKUDOOT355JPatm2b9u3bp9LS0quuOXLkiCSpqKgooQEBAJnJU4Cqqqq0ZcsW7dixQzk5Oers7JQkBQIBjRkzRidOnNCWLVv0s5/9TOPHj9fRo0e1du1azZ49W3feeWdK/gUAAOnJU4A2bdok6fIvm37d5s2btWzZMmVnZ2vPnj165ZVX1Nvbq+LiYi1evFjPPfdc0gYGAGQGz9+C+zbFxcVqaGi4poEAANeHhK+CS5VIJKJAIGA9BoaRRK6cS2SNdPnXCwAkx9WuguNmpAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACAiYT+IiowlBK5X+4wu8cugAFwBgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMDEsAsQ9/ACgMxwta/nwy5APT091iMAAJLgal/PfW6YnXL09/ervb1dOTk58vl8cc9FIhEVFxerra1Nubm5RhPa4zhcxnG4jONwGcfhsuFwHJxz6unpUSgU0ogRg5/nDLs/xzBixAhNmDDhW/fJzc29rt9gX+E4XMZxuIzjcBnH4TLr4xAIBK66z7D7FhwA4PpAgAAAJtIqQH6/X+vXr5ff77cexRTH4TKOw2Uch8s4Dpel03EYdhchAACuD2l1BgQAyBwECABgggABAEwQIACAibQJUG1trW6++WaNHj1aZWVl+uijj6xHGnIvvviifD5f3DZt2jTrsVJu//79evDBBxUKheTz+bR9+/a4551zeuGFF1RUVKQxY8aooqJCx48ftxk2ha52HJYtW3bF+2P+/Pk2w6ZITU2N7rrrLuXk5KigoEALFy5Uc3Nz3D4XLlxQVVWVxo8frxtvvFGLFy9WV1eX0cSp8V2Ow/3333/F+2HlypVGEw8sLQL0zjvvqLq6WuvXr9fHH3+sGTNmaN68eTp16pT1aEPu9ttvV0dHR2w7cOCA9Ugp19vbqxkzZqi2tnbA5zdu3KhXX31Vr7/+ug4dOqSxY8dq3rx5unDhwhBPmlpXOw6SNH/+/Lj3x9atW4dwwtRraGhQVVWVDh48qN27d6uvr09z585Vb29vbJ+1a9fq/fff17vvvquGhga1t7dr0aJFhlMn33c5DpK0YsWKuPfDxo0bjSYehEsDs2bNclVVVbGPL1265EKhkKupqTGcauitX7/ezZgxw3oMU5Lctm3bYh/39/e7YDDo/vjHP8Ye6+7udn6/323dutVgwqHxzePgnHNLly51CxYsMJnHyqlTp5wk19DQ4Jy7/N9+1KhR7t13343t869//ctJco2NjVZjptw3j4Nzzt13333uqaeeshvqOxj2Z0AXL15UU1OTKioqYo+NGDFCFRUVamxsNJzMxvHjxxUKhTRp0iQ99thjOnnypPVIplpbW9XZ2Rn3/ggEAiorK7su3x/79u1TQUGBpk6dqlWrVunMmTPWI6VUOByWJOXl5UmSmpqa1NfXF/d+mDZtmiZOnJjR74dvHoevvPXWW8rPz9f06dO1bt06nTt3zmK8QQ27m5F+0+nTp3Xp0iUVFhbGPV5YWKjPPvvMaCobZWVlqqur09SpU9XR0aENGzbo3nvv1bFjx5STk2M9nonOzk5JGvD98dVz14v58+dr0aJFKi0t1YkTJ/Sb3/xGlZWVamxsVFZWlvV4Sdff3681a9bo7rvv1vTp0yVdfj9kZ2dr3Lhxcftm8vthoOMgSY8++qhKSkoUCoV09OhRPfvss2pubtZ7771nOG28YR8g/L/KysrYP995550qKytTSUmJ/va3v2n58uWGk2E4ePjhh2P/fMcdd+jOO+/U5MmTtW/fPs2ZM8dwstSoqqrSsWPHroufg36bwY7D448/HvvnO+64Q0VFRZozZ45OnDihyZMnD/WYAxr234LLz89XVlbWFVexdHV1KRgMGk01PIwbN05TpkxRS0uL9ShmvnoP8P640qRJk5Sfn5+R74/Vq1dr586d+uCDD+L+fEswGNTFixfV3d0dt3+mvh8GOw4DKSsrk6Rh9X4Y9gHKzs7WzJkzVV9fH3usv79f9fX1Ki8vN5zM3tmzZ3XixAkVFRVZj2KmtLRUwWAw7v0RiUR06NCh6/798cUXX+jMmTMZ9f5wzmn16tXatm2b9u7dq9LS0rjnZ86cqVGjRsW9H5qbm3Xy5MmMej9c7TgM5MiRI5I0vN4P1ldBfBdvv/228/v9rq6uzn366afu8ccfd+PGjXOdnZ3Wow2pX/3qV27fvn2utbXVffjhh66iosLl5+e7U6dOWY+WUj09Pe6TTz5xn3zyiZPkXn75ZffJJ5+4zz//3Dnn3O9//3s3btw4t2PHDnf06FG3YMECV1pa6s6fP288eXJ923Ho6elxTz/9tGtsbHStra1uz5497kc/+pG79dZb3YULF6xHT5pVq1a5QCDg9u3b5zo6OmLbuXPnYvusXLnSTZw40e3du9cdPnzYlZeXu/LycsOpk+9qx6GlpcX99re/dYcPH3atra1ux44dbtKkSW727NnGk8dLiwA559xrr73mJk6c6LKzs92sWbPcwYMHrUcackuWLHFFRUUuOzvbff/733dLlixxLS0t1mOl3AcffOAkXbEtXbrUOXf5Uuznn3/eFRYWOr/f7+bMmeOam5tth06BbzsO586dc3PnznU33XSTGzVqlCspKXErVqzIuP9JG+jfX5LbvHlzbJ/z58+7J554wn3ve99zN9xwg3vooYdcR0eH3dApcLXjcPLkSTd79myXl5fn/H6/u+WWW9yvf/1rFw6HbQf/Bv4cAwDAxLD/GRAAIDMRIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACb+D0OnvTMMgAf7AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ex_img2 = cv2.imread('../images/num7.jpg')\n",
    "\n",
    "ex_img2 = cv2.cvtColor(ex_img2, cv2.COLOR_BGR2GRAY)\n",
    "ex_img2 = cv2.resize(ex_img2, (28, 28))\n",
    "ex_img2 = ex_img2.astype('float32') / 255.0\n",
    "\n",
    "plt.imshow(ex_img2, cmap='Greys_r', interpolation='nearest')\n",
    "ex_img2 = np.reshape(ex_img2, (1, 28, 28, 1))\n",
    "\n",
    "print (ex_img2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec49139f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 16ms/step\n",
      "[[1.2776567e-07 2.0073880e-04 5.8373407e-06 3.8394443e-05 7.0651424e-08\n",
      "  1.8988273e-09 8.3512788e-09 9.9975389e-01 4.0830543e-07 5.0954088e-07]]\n",
      "Classified digit is: 7\n"
     ]
    }
   ],
   "source": [
    "digit_probabilities2 = digit_recognition_model.predict(ex_img2)\n",
    "\n",
    "print(digit_probabilities2)\n",
    "\n",
    "digit2 = np.argmax(digit_probabilities2)\n",
    "\n",
    "print(\"Classified digit is: \" + str(digit2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
